---
title: "Uncertainty estimation of predictions outputs from Polynomial SVM applied to 2D simulations"
author: "Helder Carneiro"

output: 
  html_document:
    toc: true
    toc_depth: 4
    toc_float: TRUE
---

## Overview

This Rmarkdown documents contains the code and 2D simulations of the error propagation and classification uncertainty estimation using Monte Carlo simulations for two generic classes with polynomial separation classified with Polynomial SVM. 

#### Packages

```{r packages , warning=F, message=FALSE}
windowsFonts(A = windowsFont("DejaVu Sans")) 
palette("Tableau10")

library(caret) #classification using train()
library(kernlab)    
library(MASS) #generate the samples and calculate the augmented noise
library(dplyr) #data handling 
library(knitr) #knit options for the Rmarkdown report
library(kableExtra) #table options
```

#### Local sources files containing the functions

```{r functions, cache=F}
source("predict_with_uncertainty_parallel_v4.R")
source("plot_functions.R")
```

#### 2D simulations with linear separation pattern

```{r 2D data simulations , cache=F}
set.seed(123)
n_points <- 10
n_replicates <- 5
noise_sd <- 0.1
rho <- 0

cov_matrix <- matrix(c(noise_sd^2, rho * noise_sd^2,
                       rho * noise_sd^2, noise_sd^2), nrow = 2)

# Define curve generator with vertical offset
curve_points <- function(n_points, curvature = 2, offset = 0, x_range = c(-0.8, 0.8)) {
  x_vals <- seq(x_range[1], x_range[2], length.out = n_points)
  y_vals <- (x_vals)^curvature + offset
  return(data.frame(x = x_vals, y = y_vals))
}

# Replicated point generation
generate_class_curve <- function(n_points, n_replicates, centers, cov_matrix) {
  points <- matrix(0, nrow = n_points * n_replicates, ncol = 2)
  
  for (i in 1:n_points) {
    base_point <- c(centers$x[i] + rnorm(1, 0, 0.1), centers$y[i] + rnorm(1, 0, 0.1))
    
    for (j in 1:n_replicates) {
      noise <- mvrnorm(1, mu = c(0, 0), Sigma = cov_matrix)
      points[(i - 1) * n_replicates + j, ] <- base_point + noise
    }
  }
  return(points)
}

# Class 1 and Class 2 on same curve, offset vertically
curve1 <- curve_points(n_points, curvature = 2, offset = 0)
curve2 <- curve_points(n_points, curvature = 2, offset = 0.5)  # class 2 is shifted up

class1 <- generate_class_curve(n_points, n_replicates, curve1, cov_matrix)
class2 <- generate_class_curve(n_points, n_replicates, curve2, cov_matrix)

generate_class_curve <- function(n_points, n_replicates, centers, cov_matrix) {
  points <- matrix(0, nrow = n_points * n_replicates, ncol = 2)
  
  for (i in 1:n_points) {
    base_point <- c(centers$x[i] + rnorm(1, 0, 0.1), centers$y[i] + rnorm(1, 0, 0.1))
    
    for (j in 1:n_replicates) {
      noise <- mvrnorm(1, mu = c(0, 0), Sigma = cov_matrix)
      points[(i - 1) * n_replicates + j, ] <- base_point + noise
    }
  }
  return(points)
}

# Generate classes
class1 <- generate_class_curve(n_points, n_replicates, curve1, cov_matrix)
class2 <- generate_class_curve(n_points, n_replicates, curve2, cov_matrix)

data <- data.frame(
  Class = factor(rep(c("Class 1", "Class 2"), each = n_points * n_replicates)),
  Sample_ID = rep(1:(2 * n_points), each = n_replicates),
  X1 = c(class1[, 1], class2[, 1]),
  X2 = c(class1[, 2], class2[, 2])
)

# Mean center the data
data$X1 <- data$X1 - mean(data$X1)
data$X2 <- data$X2 - mean(data$X2)

# Calculate the mean and standard deviation of each sample
mean_data <- aggregate(cbind(X1, X2) ~ Class + Sample_ID, data = data, FUN = mean)
sd_data <- aggregate(cbind(X1, X2) ~ Class + Sample_ID, data = data, FUN = sd)
rm(class1, class2)

```

#### Fitting the PLS-DA model

```{r PLSDA model fitting, cache = F}
svm_model <- train(y = mean_data$Class,
                   x = mean_data[,3:4], 
                   method = "svmPoly")
svm_model
```

## Local uncertainty

The `predict_with_uncertainty_parallel()` function is used to estimate the uncertainty of each sample.

```{r uncertainty estimation }
loc_pred <- predict_with_uncertainty(data[,-1],
                                                svm_model,
                                                n_sim = 500,
                                                return_simulations = T)

```

This function returns the mean and standard deviation of the $\hat{y}$ of each sample based on the Monte Carlo simulation. It also returns the area above and bellow the decision boundary (in this case, zero) of each sample. The classification output indicates if the sample is being classified with certainty or not. For samples where the distribution crosses the decision boundary (considering the 95% confidence level), the sample will be assigned as uncertain. Once `return_simulations = T`, the augmented noise will be saved. The results for this 2D simulated data is displayed in Table 1.

```{r uncertainty results, cache = F, echo = F}

loc_pred_results <- Reduce(rbind, lapply(loc_pred, function(x) {
  Reduce(cbind, x[names(x) != "sim_X"])
}))

colnames(loc_pred_results) <- names(loc_pred[[1]])[-7]


loc_pred_results <- data.frame(class = mean_data$Class, loc_pred_results)
loc_pred_results$sample <- rep(1:10, 2)

loc_pred_results$y_mean <- round(as.numeric(loc_pred_results$y_mean), digits = 4)
loc_pred_results$y_sd <- round(as.numeric(loc_pred_results$y_sd), digits = 4)
loc_pred_results$area_above_0 <- round(as.numeric(loc_pred_results$area_above_0), digits = 3)
loc_pred_results$area_below_0 <- round(as.numeric(loc_pred_results$area_below_0), digits = 3)

kable(loc_pred_results,
      format = 'html',
      caption = "Table 1. Results of the predict_with_uncertainty_parallel() function. ") %>%
  kable_styling(bootstrap_options = c("striped", "hover"), full_width = F)
```

These results can be visualized, as showed in Figure 1.

```{r  fig.width= 7.5, fig.height=2.5, fig.align="center", dpi =600, echo = F, fig.cap= "Figure 1. Uncertainty of the classification outputs for the Polynomial SVM model. (a) 2D data simulation. Circles represents the replicates, while the triangles represents the mean of replicates. (b) Augmented noise generated through eigen decomposition of the original covariance matrix for each sample. 50 points are showed for each sample. (c) Estimated uncertainty for each sample. "}

layout_matrix <- matrix(1:3,
                        nrow = 1,
                        byrow = F)

layout(layout_matrix)

op <- par(family = "A")
par(mgp = c(1.5, 0.5, 0)) 
par(mar = c(3,3,1,1))

plot_a(data,mean_data)
 legend("bottomleft",
           legend = c("Class 1", "Class 2"),
           pch = 24,
           cex = 1,
           inset = 0.01,
           box.lwd = 0.5,
           pt.bg = c(1:2))
box(lwd = 1)
mtext("a", side = 3, line = -1.8, adj = 0.04, font = 1, cex = 1.5)

model <- svm_model
  
  x_grid <- seq(min(data$X1) - 2, max(data$X1) + 2, length.out = 1000)
  y_grid <- seq(min(data$X2) - 2, max(data$X2) + 2, length.out = 1000)
  
  grid <- expand.grid(X1 = x_grid, X2 = y_grid)
  
  grid$Decision <- predict(model, newdata = grid)
  grid_numeric <- ifelse(grid$Decision == "Class 1", 1, 2)

plot(data$X1,
       data$X2,
       type = 'n',
       xlim = c(-1,1),
       ylim = c(-1,1),
       xlab = "X1", 
       ylab = "X2",
       cex.axis = 0.8,
       col.axis="gray30",
       cex.lab = 1)
points(grid$X1,
         grid$X2,
         pch = 16,
         cex = 0.2,
         col = grid_numeric)
  points(0,
         0,
         pch = 16,
         cex = 100,
         col = scales::alpha("white",0.8))
  abline(v = 0, h = 0, col = "gray95")
  


  
  for (i in 1:nrow(mean_data)){
    p <- round(runif(250, min = 1, max = 500))
    points(loc_pred[[i]][["sim_X"]][p,1],
         loc_pred[[i]][["sim_X"]][p,2],
         pch = 16,
         cex = 0.3,
         col = scales::alpha(palette()[as.numeric(mean_data$Class)[i]], 0.5))
    
    segments(x0=rep(mean_data$X1[i], each = 50),
             y0=rep(mean_data$X2[i], each = 50),
             x1=loc_pred[[i]][["sim_X"]][p,1],
             y1=loc_pred[[i]][["sim_X"]][p,2],
             col = scales::alpha(palette()[as.numeric(mean_data$Class)[i]], 0.3),
             lwd = 1)
  }
  
  points(mean_data$X1,
         mean_data$X2,
         pch = 24,
         cex = 1.5,
         col = "gray10",
         bg = mean_data$Class)
  legend("bottomright",
           legend = c("Poly SVM Boundary"),
           lty = 2,
           cex = 0.8,
           inset = 0.01,
           box.lwd = 0.5)
legend_x <- -0.2
legend_y <- -0.95
legend_size <- 0.2

# Draw the top half (blue)
rect(legend_x, legend_y, legend_x + legend_size, legend_y + legend_size / 2, col = "#dce4ed", border = NA)

# Draw the bottom half (orange)
rect(legend_x, legend_y - legend_size / 2, legend_x + legend_size, legend_y, col = '#fce8d5', border = NA)
box(lwd = 1)
mtext("b", side = 3, line = -1.8, adj = 0.04, font = 1, cex = 1.5)

n_samples_class <- as.numeric(table(mean_data$Class))

n_samples <- max(n_samples_class)

color <- palette()[rep(1:2, each = 10)]

plot(1:n_samples,
       c(1:n_samples),
       type = "n",
       xlab = expression(italic(hat(y)), family = "Helvetica-Oblique"),
       ylab = "Samples",
       ylim= c(1,max(n_samples_class)*1.2),
       xlim = c(-2.5,2.5),
      # yaxt = c(1:max(n_samples_class)),
       cex.axis = 0.8,
       col.axis="gray30",
       cex.lab = 1
  )
  
  abline(v = 0, , col = "black", lwd = 1, lty = 2)
  
    up_lim <- loc_pred_results$y_mean + 2*loc_pred_results$y_sd
    bot_lim <- loc_pred_results$y_mean - 2*loc_pred_results$y_sd

     
    arrows(x0 = bot_lim,
           x1 = up_lim,
           y0 = rep(1:10, 2),
           y1 = rep(1:10, 2),
           col = scales::alpha(color, 0.6),
           code = 3,
           angle = 90,
           length = 0.05)
     
    points(loc_pred_results$y_mean,
           rep(1:10, 2),
           pch = 24,
           cex = 1.5,
           col = "gray10",
           bg = color)
   

mtext("c", side = 3, line = -1.8, adj = 0.04, font = 1, cex = 1.5)

```








## Correlated noise 

The previous examples showed how the uncertainty was estimated for Independent and Identically Distributed noise (iid noise). The next examples show how the uncertainty can be estimated for correlated noise, easily found in spectroscopy data.

### Positive covariance

For this example, the covariance between the X1 and X2 is 0.8. The results are showed in Table 2. 


```{r 2D data simulations 2 , cache=F, echo = F}
set.seed(1234)
n_points <- 10
n_replicates <- 5
noise_sd <- 0.1
rho <- 0.8

cov_matrix <- matrix(c(noise_sd^2, rho * noise_sd^2,
                       rho * noise_sd^2, noise_sd^2), nrow = 2)

# Define curve generator with vertical offset
curve_points <- function(n_points, curvature = 2, offset = 0, x_range = c(-0.8, 0.8)) {
  x_vals <- seq(x_range[1], x_range[2], length.out = n_points)
  y_vals <- (x_vals)^curvature + offset
  return(data.frame(x = x_vals, y = y_vals))
}

# Replicated point generation
generate_class_curve <- function(n_points, n_replicates, centers, cov_matrix) {
  points <- matrix(0, nrow = n_points * n_replicates, ncol = 2)
  
  for (i in 1:n_points) {
    base_point <- c(centers$x[i] + rnorm(1, 0, 0.1), centers$y[i] + rnorm(1, 0, 0.1))
    
    for (j in 1:n_replicates) {
      noise <- mvrnorm(1, mu = c(0, 0), Sigma = cov_matrix)
      points[(i - 1) * n_replicates + j, ] <- base_point + noise
    }
  }
  return(points)
}

# Class 1 and Class 2 on same curve, offset vertically
curve1 <- curve_points(n_points, curvature = 2, offset = 0)
curve2 <- curve_points(n_points, curvature = 2, offset = 0.5)  # class 2 is shifted up

class1 <- generate_class_curve(n_points, n_replicates, curve1, cov_matrix)
class2 <- generate_class_curve(n_points, n_replicates, curve2, cov_matrix)
# Apply jitter to simulate biological/technical variation
generate_class_curve <- function(n_points, n_replicates, centers, cov_matrix) {
  points <- matrix(0, nrow = n_points * n_replicates, ncol = 2)
  
  for (i in 1:n_points) {
    base_point <- c(centers$x[i] + rnorm(1, 0, 0.1), centers$y[i] + rnorm(1, 0, 0.1))
    
    for (j in 1:n_replicates) {
      noise <- mvrnorm(1, mu = c(0, 0), Sigma = cov_matrix)
      points[(i - 1) * n_replicates + j, ] <- base_point + noise
    }
  }
  return(points)
}

# Generate classes
class1 <- generate_class_curve(n_points, n_replicates, curve1, cov_matrix)
class2 <- generate_class_curve(n_points, n_replicates, curve2, cov_matrix)

data <- data.frame(
  Class = factor(rep(c("Class 1", "Class 2"), each = n_points * n_replicates)),
  Sample_ID = rep(1:(2 * n_points), each = n_replicates),
  X1 = c(class1[, 1], class2[, 1]),
  X2 = c(class1[, 2], class2[, 2])
)

# Mean center the data
data$X1 <- data$X1 - mean(data$X1)
data$X2 <- data$X2 - mean(data$X2)

# Calculate the mean and standard deviation of each sample
mean_data <- aggregate(cbind(X1, X2) ~ Class + Sample_ID, data = data, FUN = mean)
sd_data <- aggregate(cbind(X1, X2) ~ Class + Sample_ID, data = data, FUN = sd)
rm(class1, class2)

svm_model <- train(y = mean_data$Class,
                   x = mean_data[,3:4], 
                   method = "svmPoly")

loc_pred <- predict_with_uncertainty(data[,-1],
                                                svm_model,
                                                n_sim = 500,
                                                return_simulations = T)


```

```{r}
svm_model
```

```{r uncertainty results 2, cache = F, echo = F}

loc_pred_results <- Reduce(rbind, lapply(loc_pred, function(x) {
  Reduce(cbind, x[names(x) != "sim_X"])
}))

colnames(loc_pred_results) <- names(loc_pred[[1]])[-7]


loc_pred_results <- data.frame(class = mean_data$Class, loc_pred_results)
loc_pred_results$sample <- rep(1:10, 2)

loc_pred_results$y_mean <- round(as.numeric(loc_pred_results$y_mean), digits = 4)
loc_pred_results$y_sd <- round(as.numeric(loc_pred_results$y_sd), digits = 4)
loc_pred_results$area_above_0 <- round(as.numeric(loc_pred_results$area_above_0), digits = 3)
loc_pred_results$area_below_0 <- round(as.numeric(loc_pred_results$area_below_0), digits = 3)

kable(loc_pred_results,
      format = 'html',
      caption = "Table 2. Results of the predict_with_uncertainty_parallel() function for variables with positive covariance. ") %>%
  kable_styling(bootstrap_options = c("striped", "hover"), full_width = F)
```


The results can also be visualized, as showed in Figure 2.

```{r  fig.width= 7.5, fig.height=2.5, fig.align="center", dpi =300, echo = F, fig.cap= "Figure 3. Uncertainty of the classification outputs for the Polynomial SVM model with positive covariance. (a) 2D data simulation. Circles represents the replicates, while the triangles represents the mean of replicates. (b) Augmented noise generated through eigen decomposition of the original covariance matrix for each sample. 50 points are showed for each sample. (c) Estimated uncertainty for each sample."}

layout_matrix <- matrix(1:3,
                        nrow = 1,
                        byrow = F)

layout(layout_matrix)

op <- par(family = "A")
par(mgp = c(1.5, 0.5, 0)) 
par(mar = c(3,3,1,1))

plot_a(data,mean_data)
 legend("bottomleft",
           legend = c("Class 1", "Class 2"),
           pch = 24,
           cex = 1,
           inset = 0.01,
           box.lwd = 0.5,
           pt.bg = c(1:2))
box(lwd = 1)
mtext("a", side = 3, line = -1.8, adj = 0.04, font = 1, cex = 1.5)

model <- svm_model
  
  x_grid <- seq(min(data$X1) - 2, max(data$X1) + 2, length.out = 1000)
  y_grid <- seq(min(data$X2) - 2, max(data$X2) + 2, length.out = 1000)
  
  grid <- expand.grid(X1 = x_grid, X2 = y_grid)
  
  grid$Decision <- predict(model, newdata = grid)
  grid_numeric <- ifelse(grid$Decision == "Class 1", 1, 2)

plot(data$X1,
       data$X2,
       type = 'n',
       xlim = c(-1,1),
       ylim = c(-1,1),
       xlab = "X1", 
       ylab = "X2",
       cex.axis = 0.8,
       col.axis="gray30",
       cex.lab = 1)
points(grid$X1,
         grid$X2,
         pch = 16,
         cex = 0.2,
         col = grid_numeric)
  points(0,
         0,
         pch = 16,
         cex = 100,
         col = scales::alpha("white",0.8))
  abline(v = 0, h = 0, col = "gray95")
  


  
  for (i in 1:nrow(mean_data)){
    p <- round(runif(250, min = 1, max = 500))
    points(loc_pred[[i]][["sim_X"]][p,1],
         loc_pred[[i]][["sim_X"]][p,2],
         pch = 16,
         cex = 0.3,
         col = scales::alpha(palette()[as.numeric(mean_data$Class)[i]], 0.5))
    
    segments(x0=rep(mean_data$X1[i], each = 50),
             y0=rep(mean_data$X2[i], each = 50),
             x1=loc_pred[[i]][["sim_X"]][p,1],
             y1=loc_pred[[i]][["sim_X"]][p,2],
             col = scales::alpha(palette()[as.numeric(mean_data$Class)[i]], 0.3),
             lwd = 1)
  }
  
  points(mean_data$X1,
         mean_data$X2,
         pch = 24,
         cex = 1.5,
         col = "gray10",
         bg = mean_data$Class)
  legend("bottomright",
           legend = c("Poly SVM Boundary"),
           lty = 2,
           cex = 0.8,
           inset = 0.01,
           box.lwd = 0.5)
legend_x <- -0.2
legend_y <- -0.95
legend_size <- 0.2

# Draw the top half (blue)
rect(legend_x, legend_y, legend_x + legend_size, legend_y + legend_size / 2, col = "#dce4ed", border = NA)

# Draw the bottom half (orange)
rect(legend_x, legend_y - legend_size / 2, legend_x + legend_size, legend_y, col = '#fce8d5', border = NA)
box(lwd = 1)
mtext("b", side = 3, line = -1.8, adj = 0.04, font = 1, cex = 1.5)

n_samples_class <- as.numeric(table(mean_data$Class))

n_samples <- max(n_samples_class)

color <- palette()[rep(1:2, each = 10)]

plot(1:n_samples,
       c(1:n_samples),
       type = "n",
       xlab = expression(italic(hat(y)), family = "Helvetica-Oblique"),
       ylab = "Samples",
       ylim= c(1,max(n_samples_class)*1.2),
       xlim = c(-2.5,2.5),
      # yaxt = c(1:max(n_samples_class)),
       cex.axis = 0.8,
       col.axis="gray30",
       cex.lab = 1
  )
  
  abline(v = 0, , col = "black", lwd = 1, lty = 2)
  
    up_lim <- loc_pred_results$y_mean + 2*loc_pred_results$y_sd
    bot_lim <- loc_pred_results$y_mean - 2*loc_pred_results$y_sd

     
    arrows(x0 = bot_lim,
           x1 = up_lim,
           y0 = rep(1:10, 2),
           y1 = rep(1:10, 2),
           col = scales::alpha(color, 0.6),
           code = 3,
           angle = 90,
           length = 0.05)
     
    points(loc_pred_results$y_mean,
           rep(1:10, 2),
           pch = 24,
           cex = 1.5,
           col = "gray10",
           bg = color)
   

mtext("c", side = 3, line = -1.8, adj = 0.04, font = 1, cex = 1.5)


```

### Negative covariance

For this example, the covariance between the X1 and X2 is -0.8. The results are showed in Table 3. 


```{r 2D data simulations 3 , cache=F, echo = F}
set.seed(1234)
n_points <- 10
n_replicates <- 5
noise_sd <- 0.1
rho <- -0.8

cov_matrix <- matrix(c(noise_sd^2, rho * noise_sd^2,
                       rho * noise_sd^2, noise_sd^2), nrow = 2)

# Define curve generator with vertical offset
curve_points <- function(n_points, curvature = 2, offset = 0, x_range = c(-0.8, 0.8)) {
  x_vals <- seq(x_range[1], x_range[2], length.out = n_points)
  y_vals <- (x_vals)^curvature + offset
  return(data.frame(x = x_vals, y = y_vals))
}

# Replicated point generation
generate_class_curve <- function(n_points, n_replicates, centers, cov_matrix) {
  points <- matrix(0, nrow = n_points * n_replicates, ncol = 2)
  
  for (i in 1:n_points) {
    base_point <- c(centers$x[i] + rnorm(1, 0, 0.1), centers$y[i] + rnorm(1, 0, 0.1))
    
    for (j in 1:n_replicates) {
      noise <- mvrnorm(1, mu = c(0, 0), Sigma = cov_matrix)
      points[(i - 1) * n_replicates + j, ] <- base_point + noise
    }
  }
  return(points)
}

# Class 1 and Class 2 on same curve, offset vertically
curve1 <- curve_points(n_points, curvature = 2, offset = 0)
curve2 <- curve_points(n_points, curvature = 2, offset = 0.5)  # class 2 is shifted up

class1 <- generate_class_curve(n_points, n_replicates, curve1, cov_matrix)
class2 <- generate_class_curve(n_points, n_replicates, curve2, cov_matrix)
# Apply jitter to simulate biological/technical variation
generate_class_curve <- function(n_points, n_replicates, centers, cov_matrix) {
  points <- matrix(0, nrow = n_points * n_replicates, ncol = 2)
  
  for (i in 1:n_points) {
    base_point <- c(centers$x[i] + rnorm(1, 0, 0.1), centers$y[i] + rnorm(1, 0, 0.1))
    
    for (j in 1:n_replicates) {
      noise <- mvrnorm(1, mu = c(0, 0), Sigma = cov_matrix)
      points[(i - 1) * n_replicates + j, ] <- base_point + noise
    }
  }
  return(points)
}

# Generate classes
class1 <- generate_class_curve(n_points, n_replicates, curve1, cov_matrix)
class2 <- generate_class_curve(n_points, n_replicates, curve2, cov_matrix)

data <- data.frame(
  Class = factor(rep(c("Class 1", "Class 2"), each = n_points * n_replicates)),
  Sample_ID = rep(1:(2 * n_points), each = n_replicates),
  X1 = c(class1[, 1], class2[, 1]),
  X2 = c(class1[, 2], class2[, 2])
)

# Mean center the data
data$X1 <- data$X1 - mean(data$X1)
data$X2 <- data$X2 - mean(data$X2)

# Calculate the mean and standard deviation of each sample
mean_data <- aggregate(cbind(X1, X2) ~ Class + Sample_ID, data = data, FUN = mean)
sd_data <- aggregate(cbind(X1, X2) ~ Class + Sample_ID, data = data, FUN = sd)
rm(class1, class2)

svm_model <- train(y = mean_data$Class,
                   x = mean_data[,3:4], 
                   method = "svmPoly")

loc_pred <- predict_with_uncertainty(data[,-1],
                                                svm_model,
                                                n_sim = 500,
                                                return_simulations = T)


```
.
```{r}
svm_model
```

.
```{r uncertainty results 3, cache = F, echo = F}

loc_pred_results <- Reduce(rbind, lapply(loc_pred, function(x) {
  Reduce(cbind, x[names(x) != "sim_X"])
}))

colnames(loc_pred_results) <- names(loc_pred[[1]])[-7]


loc_pred_results <- data.frame(class = mean_data$Class, loc_pred_results)
loc_pred_results$sample <- rep(1:10, 2)

loc_pred_results$y_mean <- round(as.numeric(loc_pred_results$y_mean), digits = 4)
loc_pred_results$y_sd <- round(as.numeric(loc_pred_results$y_sd), digits = 4)
loc_pred_results$area_above_0 <- round(as.numeric(loc_pred_results$area_above_0), digits = 3)
loc_pred_results$area_below_0 <- round(as.numeric(loc_pred_results$area_below_0), digits = 3)

kable(loc_pred_results,
      format = 'html',
      caption = "Table 3. Results of the predict_with_uncertainty_parallel() function for variables with negative covariance. ") %>%
  kable_styling(bootstrap_options = c("striped", "hover"), full_width = F)
```


The results can also be visualized, as showed in Figure 3.

```{r  fig.width= 7.5, fig.height=2.5, fig.align="center", dpi =300, echo = F, fig.cap= "Figure 3. Uncertainty of the classification outputs for the Polynomial SVM model with negative covariance. (a) 2D data simulation. Circles represents the replicates, while the triangles represents the mean of replicates. (b) Augmented noise generated through eigen decomposition of the original covariance matrix for each sample. 50 points are showed for each sample. (c) Estimated uncertainty for each sample."}

layout_matrix <- matrix(1:3,
                        nrow = 1,
                        byrow = F)

layout(layout_matrix)

op <- par(family = "A")
par(mgp = c(1.5, 0.5, 0)) 
par(mar = c(3,3,1,1))

plot_a(data,mean_data)
 legend("bottomleft",
           legend = c("Class 1", "Class 2"),
           pch = 24,
           cex = 1,
           inset = 0.01,
           box.lwd = 0.5,
           pt.bg = c(1:2))
box(lwd = 1)
mtext("a", side = 3, line = -1.8, adj = 0.04, font = 1, cex = 1.5)

model <- svm_model
  
  x_grid <- seq(min(data$X1) - 2, max(data$X1) + 2, length.out = 1000)
  y_grid <- seq(min(data$X2) - 2, max(data$X2) + 2, length.out = 1000)
  
  grid <- expand.grid(X1 = x_grid, X2 = y_grid)
  
  grid$Decision <- predict(model, newdata = grid)
  grid_numeric <- ifelse(grid$Decision == "Class 1", 1, 2)

plot(data$X1,
       data$X2,
       type = 'n',
       xlim = c(-1,1),
       ylim = c(-1,1),
       xlab = "X1", 
       ylab = "X2",
       cex.axis = 0.8,
       col.axis="gray30",
       cex.lab = 1)
points(grid$X1,
         grid$X2,
         pch = 16,
         cex = 0.2,
         col = grid_numeric)
  points(0,
         0,
         pch = 16,
         cex = 100,
         col = scales::alpha("white",0.8))
  abline(v = 0, h = 0, col = "gray95")
  


  
  for (i in 1:nrow(mean_data)){
    p <- round(runif(250, min = 1, max = 500))
    points(loc_pred[[i]][["sim_X"]][p,1],
         loc_pred[[i]][["sim_X"]][p,2],
         pch = 16,
         cex = 0.3,
         col = scales::alpha(palette()[as.numeric(mean_data$Class)[i]], 0.5))
    
    segments(x0=rep(mean_data$X1[i], each = 50),
             y0=rep(mean_data$X2[i], each = 50),
             x1=loc_pred[[i]][["sim_X"]][p,1],
             y1=loc_pred[[i]][["sim_X"]][p,2],
             col = scales::alpha(palette()[as.numeric(mean_data$Class)[i]], 0.3),
             lwd = 1)
  }
  
  points(mean_data$X1,
         mean_data$X2,
         pch = 24,
         cex = 1.5,
         col = "gray10",
         bg = mean_data$Class)
  legend("bottomright",
           legend = c("Poly SVM Boundary"),
           lty = 2,
           cex = 0.8,
           inset = 0.01,
           box.lwd = 0.5)
legend_x <- -0.2
legend_y <- -0.95
legend_size <- 0.2

# Draw the top half (blue)
rect(legend_x, legend_y, legend_x + legend_size, legend_y + legend_size / 2, col = "#dce4ed", border = NA)

# Draw the bottom half (orange)
rect(legend_x, legend_y - legend_size / 2, legend_x + legend_size, legend_y, col = '#fce8d5', border = NA)
box(lwd = 1)
mtext("b", side = 3, line = -1.8, adj = 0.04, font = 1, cex = 1.5)

n_samples_class <- as.numeric(table(mean_data$Class))

n_samples <- max(n_samples_class)

color <- palette()[rep(1:2, each = 10)]

plot(1:n_samples,
       c(1:n_samples),
       type = "n",
       xlab = expression(italic(hat(y)), family = "Helvetica-Oblique"),
       ylab = "Samples",
       ylim= c(1,max(n_samples_class)*1.2),
       xlim = c(-2.5,2.5),
      # yaxt = c(1:max(n_samples_class)),
       cex.axis = 0.8,
       col.axis="gray30",
       cex.lab = 1
  )
  
  abline(v = 0, , col = "black", lwd = 1, lty = 2)
  
    up_lim <- loc_pred_results$y_mean + 2*loc_pred_results$y_sd
    bot_lim <- loc_pred_results$y_mean - 2*loc_pred_results$y_sd

     
    arrows(x0 = bot_lim,
           x1 = up_lim,
           y0 = rep(1:10, 2),
           y1 = rep(1:10, 2),
           col = scales::alpha(color, 0.6),
           code = 3,
           angle = 90,
           length = 0.05)
     
    points(loc_pred_results$y_mean,
           rep(1:10, 2),
           pch = 24,
           cex = 1.5,
           col = "gray10",
           bg = color)
   

mtext("c", side = 3, line = -1.8, adj = 0.04, font = 1, cex = 1.5)


```